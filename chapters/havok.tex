\chapter{Modeling Local Particulate Matter Dynamics with Time Delay Embeddings}\label{ch:havok}
% \chapter{Time-delay Embedding Models for Particulate Matter - Outlier Detection and Forecasting}\label{ch:havok}

The low-cost sensor network detailed in Chapter~\ref{ch:air-network} collects a
continuous stream of air quality data for a plethora of locations with high
temporal resolution ranging between $0.1$ to $1.0$ Hz. The real-time
visualization dashboards developed for the network offer immediate insight into
local air conditions. However, the challenge remains to extract actionable
insights from the large amounts of historical data. For example, we are concerned
with answering simple questions such as: \textit{What is the typical pattern of
  PM variability at this location}, \textit{Are changes in PM gradual or due to
  identifiable events?}, and \textit{Can we use historical PM measurements to
  provide insight into future air quality trends?}.

In this chapter, we propose a physics-based model rooted in Koopman operator
theory to address these questions by accurately capturing local PM dynamics.
Specifically, we extend the \textit{Hankel Alternative View of Koopman} (HAVOK)
framework introduced by Brunton et al. to apply to time series measurements of
PM data. In this data-driven approach, PM dynamics are described via a simple
linear system with aperiodic external forcing. The forcing function extracted
from the model provides a clear method to identify abrupt pollution events from
historical data. The model also provides the means for  accurate short-term
forecasts. Importantly, HAVOK models require few parameters, can be fit
efficiently using established numerical linear algebra routines, and are small
enough to be readily deployed directly onto sensor hardware.


\section{Motivation}


Due to a combination of both natural and anthropogenic dynamics, particulate
matter generally follows well observed patterns which include seasonal, weekly,
and diurnal cycles. For example, researchers in Beijing, China analyzed PM 2.5
measurements in both urban and rural settings finding significant diurnal
trends \cite{pm-patterns-china}. In urban areas, daily PM 2.5 distributions
tended to be bimodal with a first peak between 7:00 - 8:00 am corresponding to
morning traffic and a second evening peak between 7:00 and 11:00 pm due to the
gradual decrease in boundary layer hight combined with evening rush-hour
activity. In rural environments, the distribution tended to be unimodal, with
significant peaks between 5:00 and 11:00 pm. Similar patterns have been observed
in other parts of world as well. For example, PM observations in Augsburg,
Germany revealed a similar unimodal distribution with peaks in the mid to late
afternoon \cite{pm-patterns-germany}. In the heavily trafficked New York City,
PM 2.5 measurements across 20 sensing stations again revealed a strong bimodal
distribution with both morning and evening peaks strongly correlated with daily
traffic trends. Furthermore, the daily PM distribution changed during weekends
reflecting decreased workplace traffic \cite{pm-patterns-nyc}. Together, these
findings suggest that PM trends are, in principle, predictable at daily and
weekly levels.

With the growing interest in air quality and its impact on global change and
human well-being, numerous models have been developed to describe PM dynamics.
These models broadly fall into three categories:
\begin{itemize}
  \item \textit{physical models} which characterize relevant phsical and
    chemical processes that generate and transport PM
  \item \textit{statistical} and \textit{machine learning} models such as recurrent neural networks and autoregressive
    integrated moving average (ARIMA) which use historical observations to build
    short-term forecasts for generic time series data
  \item \textit{hybrid models} which augment physical models with machine
    learning to blend the two.
\end{itemize}

Detailed physical models for PM modeling and forecasting are based on
comprehensive descriptions of all relevant atmospheric processes including
emission, transport, and chemical reactions. Among these, one of the most widely
adopted methods is the Chemical Transport Model (CTM). These models directly simulate
the movement of air masses together with the complex chemical reactions that occur in the
atmosphere. The US Environmental Protection Agency's Community Multiscale Air
Quality (CMAQ) model is one such example which combines a meteorological
modeling system together with emissions inventories and chemical transport
\cite{cmaq-overview}. Clearly, the accuracy of these models depends
significantly on the resolution of the incorporated meteorological analysis
(typically grids have resolutions on the order of $\sim$1 km) \textit{and}
the accuracy of emission inventories which must be constantly updated to reflect
evolving human activities. Furthermore, these approaches can become highly
computationally expensive and are sensitive to uncertainties in input data
and model parameters.

A similar approach worth mentioning is transport models which focus specifically
on capturing the movement of PM and other pollutants through the atmosphere.
\textit{Eulerian} models use a fixed grid to solve the advection-diffusion
equation by computing dynamics of PM concentrations as particulates spread
between neighboring grid cells. Alternatively, \textit{Lagrangian} models
simulate the motion of ensembles of individual particles by integrating wind
velocity data from meteorological analyses. Generally Eulerian models are
preferred when modeling large-scale, continuous dynamics while the Lagrangian
approach is good at capturing localized point-source pollution events. The
\textit{Hybrid Single-Particle Lagrangian Integrated Trajectory} (HYSPLIT) model
developed by US National Oceanic and Atmospheric Administration (NOAA) blends
the two approaches to simultaneously capture air parcel trajectories and
vertical/horizontal pollutant dispersion
\cite{hysplit-overview}.

Despite their utility, the complexity of running these large scale physical
models has resulted in limited application to the analysis of local,
ground-level PM measurements. The increasing availability of low-cost PM data
coupled with the recent explosion in popularity of machine learning has led
many researchers to investigate alternative data-driven methods for PM
analysis. One popular approach is to leverage ground-level, \textit{in situ}
observations together with coincident satellite imagery to combine remotely sensed
parameters like \textit{aerosol optical depth} (AOD) together with
meteorological data to develop large-scale maps for PM concentrations
\cite{prabuddha-pm-satellite}. While this approach can yield maps
of the large-scale PM distribution, it faces similar limitations to the retrieval of
water quality parameters previously outlined in
Chapter~\ref{ch:robot-team-supervised}: large quantities of satellite data
coincident with ground level sensors are needed, and the spatio-temporal
resolution of the resulting maps is limited by the scale and revisit times of the
satellites.

A distinct approach is to consider historical PM measurements as abstract time
series to which general statistical and machine learning methods such as
ARIMA models and recurrent neural networks (RNN) and can be applied
\cite{intro-to-time-series-models, time-series-rnns}.
Iskandaryan et al. reviewed over 316 papers on machine learning approaches for
air quality time series and identified neural networks, regression
models, and decision trees as the most popularly employed techniques.
Additionally, they note that 66\% of the studies reviewed utilized data sampled
at an hourly rate \cite{iskandaryan-2020}.

While physical models like CMAQ and HYSPLIT are limited by the accuracy of
emission inventories and the resolution of meteorological data, machine learning
approaches suffer from two further complications: model size and model
interpretability. The enormous size of modern deep neural networks with
many thousands to even billions of parameters leads to lengthy training times and limits
our capability to deploy models on resource constrained systems like the
embedded computers used in low-cost PM sensors. Furthermore, the small-scale
spatial variability of PM patterns means that individual models must be trained for
\textit{each} PM sensor. The black-box nature of most machine
learning algorithms makes it challenging to asses the ability for a trained
model to generalize beyond the scope of the supplied training data.  On the other
hand, regression based statistical models like ARIMA are not flexible enough to
adapt to intermittent, discrete pollution events (e.g. a car briefly parked
under a sensor) which result rare, but significant spikes in PM concentration.

For these reasons we seek to develop a hybrid approach which provides a
physically motivated model leveraging historical PM observations to
accurately describe PM dynamics. Specifically, the widely observed diurnal patterns
of PM data referenced above suggest that \textit{most of the time}, PM
follows a simple, predictable pattern of a slow diurnal cycle. However, high
frequency observations facilitated by the IPS7100 sensor included in the
low-cost sensor network described in Chapter~\ref{ch:air-network} suggest that
intermittent bursts corresponding to rare pollution episodes may be equally
important for an accurate description of PM. Given that EPA regulations for PM
are limited to a temporal scale of 24-hour daily averages \cite{epa-standards},
short spikes in PM concentration can easily be \textit{washed out} during the
averaging process underestimating potential PM exposures. If they occur during
times of increased human traffic, these acute pollution events may lead to a
significant increase in accumulated PM exposures. As a particularly poignant
example, significant increases in child PM exposures have been identified with
diesel-powered school busses during drop off and pick up times
\cite{school-pm-exposure}.

In the remainder of this chapter, explore the \textit{Hankel Alternative View of
Koopman} (HAVOK) framework developed by Brunton et al. for modeling dynamical systems
\cite{brunton-havok-orig}. The HAVOK model is based on Koopman Operator theory
and identifies relevant state variables using time-delay embeddings whose
dynamics are \textit{mostly} linear with
intermittent external forcing. In their original work, Brunton et al.
demonstrated this approach can effectively model chaotic dynamical
systems. However, like many other recently developed data-driven models, it has yet to
see wide application to realistic, noisy datasets. For our purposes, the model
is particularly attractive as the separation of the dynamical model into linear
and external forcing contributions aligns well with the observed diurnal trends
of PM data with occasional spikes corresponding to rare high-pollution events.
First we provide a detailed overview of Koopman operator theory which motivates
the HAVOK model. We then additionally outline recent work connecting HAVOK to
the local differential geometry of parametric curves via the Frenet-Serret
frame. After demonstrating the method on a toy problem (the Lorenz attractor),
we describe our contributions which enable application of the HAVOK model to
noisy PM time series and extend the model to enable short term forecasts.


\section{Koopman Operator Theory}

To develop a time series model to describe PM, let us first consider a generic
\textit{dynamical system} described by a differential equation of the form
\begin{equation}
  \frac{d}{dt} \mathbf{x}(t) = \mathbf{f}(\mathbf{x}(t))
\end{equation}
where the vector $\mathbf{x}$ denotes all of the variables describing the state of
a system and the vector-valued function $\mathbf{f}$ governs the (potentially nonlinear)
dynamics which describe the time evolution of the system's state. As a concrete
example, we may consider a single particle governed by Newton's laws of motion.
In this notation, the state vector $\mathbf{x}=[x_1, x_2, x_3, v_1, v_2, v_3]^T$ contains
three position and three velocity components while $\mathbf{f}=[v_1,v_2,v_3, f_1/m,
f_2/m, f_3/m]^T$ connects the derivative of position to each velocity and each
velocity to the components of the net force applied to the particle along each
axis reduced by a factor of the particle's mass.

To model the dynamics of a system, one must first identify the relevant state
variables together with their associated dynamical law. Then, in principle, the
solution can be obtained via integration. If the initial state is
$\mathbf{x}_0$, then at a time $\tau$, we have
\begin{equation}
  \mathbf{x}(\tau) = \mathbf{x}_0 + \int_0^{\tau} \mathbf{f}(\mathbf{x}(\tau)) d\tau = \mathbf{F}^{\tau}(\mathbf{x}_0)
\end{equation}
where we have identified $\mathbf{F}^\tau$ as the \textit{flow map} of time
$\tau$. For discrete systems, as is commonly encountered for systems sampled at
discrete intervals, we may instead write
\begin{equation}
  \mathbf{x}_{k+1} = \mathbf{F}(\mathbf{x}_k)
\end{equation}
where $\mathbf{x}_k$ is the state at time $t=t_k$.

For even simple dynamical systems, nonlinear $\mathbf{f}$ often prevent analytic
solutions for $\mathbf{x}(t)$. As a consequence, much of mathematical physics is
concerned with finding equivalent formulations for the dynamical system which
admit simpler equations whose solutions can be easily obtained. For example, for
systems with conserved Hamiltonians, canonical transformations can produce new
coordinates which are all cyclic, thereby yielding trivial solutions for the
time evolution of the system \cite{goldstein}. Despite the beauty of these
results, deriving valid transformations of this sort often requires as much (if
not more) effort as solving the original nonlinear system itself.

A different perspective can be obtained by instead considering quantum
mechanics where a system is described by an abstract quantum
mechanical state vector $\lvert \psi \rangle$. The state is not directly
measured, rather the results of physical measurements (i.e. observables) are
encoded as the
eigenvalues of Hermitian operators. Interestingly there exist two equivalent
descriptions for the time evolution of a quantum system. In the Schr\"{o}dinger
picture, one evolves the state vector $\lvert  \psi(t) \rangle$ via the
Schr\"{o}dinger equation with operators corresponding to observables fixed
in time. In the Heisenberg picture, states are instead frozen while the operators
corresponding to observables are time-evolved \cite[pgs 80-84]{sakurai}.
Motivated by this equivalence, Koopman sought to develop an operator theoretic
approach to dynamical systems describing the evolution of observables of a
dynamical system \cite{koopman-orig}.

For dynamical systems, we define an observable as a map $y:\mathcal{M}\to\R$
which sends states $\mathbf{x}$ in a state-space manifold $\mathcal{M}$ to
numbers in $\R$. For instance, the function $y(\mathbf{x})=x_1$, which selects
the first component of $\mathbf{x}$ is a valid observable. For the example of a
single particle obeying Newton's laws, the kinetic energy
\begin{equation}
  T(\mathbf{x}) = \frac{1}{2}m \left(v_1^2+v_2^2+v_3^2\right),
\end{equation}
is also a valid observable. The Koopman operator, $\mathcal{K}$, is then defined
as an operator acting on the (infinite-dimensional) space of observables so that
\begin{equation}
  \mathcal{K}y(\mathbf{x}_k) = y \circ \mathbf{F}(\mathbf{x}_k) = y(\mathbf{x}_{k+1}).
\end{equation}
That is, $\mathcal{K}$ applied to an observable yields the value of that
observable for the time-evolved state. If we denote $y(\mathbf{x}_k)=y_k$, then
the Koopman operators defines a linear dynamical system on the space of
observables:
\begin{equation}
  y_{k+1} = \mathcal{K}y_k.
\end{equation}

This remarkable result suggests that if we can identify the Koopman operator
for the observables of a nonlinear dynamical system, then the time evolution of
\textit{all} observables can be easily described by a linear system. At first
glance, this is excellent because for our purposes the \textit{things we actually
care about} for air quality are observables, e.g. the PM concentrations at
various size fractions. We do not easily have access to the full sate vector
describing the comprehensive state of the air localized around a monitoring
device. However, by shifting our focus from the finite, nonlinear dynamics
of the system state to the observables, we have obtained a linear dynamical
system in an infinite dimensional Hilbert space. Surely, solving for
$\mathcal{K}$ should as (if not more) difficult as solving the original system.

Consider a subspace of the space of observables spanned by the functions $\left\{
  y_1, ... y_k  \right\}$. Then if the Koopman is closed on this space so that
$\mathcal{K}(\sum_i^k a_i y_i) = \sum_j^k b_jy_j$ then we may extract a finite
matrix representation of $\mathcal{K}$ for observables in this subspace. 


\section{Hankel Alternative View Of Koopman}



\section{The Frenet-Serret Frame and sHAVOK}


% - Koopman operator theory for dynamical systems \cite{brunton-koopman-theory}
% - Original HAVOK paper \cite{brunton-havok-orig}.
% - HAVOK paper w/ ML for forcing \cite{havok-ml}
% - sHAVOK and connections to frenet-serret frame \cite{havok-diffgeo}


% \subsection{Global Motivation: Koopman Operator Theory}

% \subsection{Time-Delay Embeddings and Taken's Theorem}
% \subsection{Hankel Alternative View of Koopman}
% \subsection{Local Motivation: Frenet-Serret Frame}
% \subsection{Structured HAVOK}
% \subsection{Simple Example: Lorenz System}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/2__lorenz-orig.png}
  \caption{(\textbf{a}) Time series for the $x$, $y$, and $z$ components of the
    Lorenz system. (\textbf{b}) 3-dimensional visualization of the attractor
    formed from $x$, $y$, and $z$ components. We note that there are two clear
    lobes corresponding to the two attractors of the system.}
  \label{fig:lorenz-time-series-orig}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\columnwidth]{havok/0-havok-lorenz/1b_A-B-sHAVOK.pdf}
  \caption{Fitted operators $\mathbf{A}$ and $\mathbf{B}$ for the Lorenz system
    learned by the HAVOK model corresponding to the linear dynamics and
    intermittent forcing.}
  \label{fig:lorenz-A-B-heatmap}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/3__havok-embedding.pdf}
  \caption{(\textbf{a}) Time series for first 3 components $v\_i$ of the HAVOK
    model. Original time series for each component are shown in blue. Time
    series predicted by the HAVOK model are overlaid in red. (\textbf(b)) The
    attractor formed by the first 3 embedding coordinates. By Taken's theorem,
    this attractor is diffeomorphic to the original attractor.}
  \label{fig:lorenz-havok-embedding}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\columnwidth]{havok/0-havok-lorenz/6__forcing-statistics.pdf}
  \caption{Proabability density function for forcing time series learned by the
    HAVOK model. The PDF is compared to a Gaussian fit for the same data. The
    wide tails of the PDF reflect that the forcing is intermittent.}
  \label{fig:lorenz-forcing-stats}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/7__attractor-w-forcing.pdf}
  \caption{(\textbf{a}) The original time series $x(t)$ plotted with the squared
  forcing time series $v_r(t)$. Red colors indicate forcing above a chosen
  threshold which accurately identify lobe-switching events. (\textbf{b}) The
  Lorenz attractor colored using the same scheme.}
  \label{fig:lorenz-attractor-forcing}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\columnwidth]{havok/0-havok-lorenz/9__timeseries-reconstruction.pdf}
  \caption{The reconstructed time series for $x(t)$ using the learned HAVOK model.}
  \label{fig:lorenz-reconstruction}
\end{figure}




\section{Study Overview}

- Describe dataset, e.g. the specific central node, focus on PM 2.5, and the
collection period (find continuous data during no-rain period during the summer
of 2023 to evaluate HAVOK model independent of problems introduced by
hygroscopic growth)


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{havok/1-havok-pm/1__timeseries-full-short.pdf}
  \caption{Time series of PM $2.5$ measurements from Central Hub 4 located in
    Planeo, Tx starting on 2023-08-04. This time series was selected during the
    long period of no precipitation during 2023 in order to limit the impact of
    hygroscopic growth on observed values. Note the occasional, thin vertical
    spikes corresponding to acute pollution events.}
  \label{fig:pm-timeseries-orig}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\columnwidth]{havok/1-havok-pm/1__pca-explained-variance.pdf}
  \caption{Explained variance of components from a principal component
    decomposition of the PM 2.5 time series sorted in decreasing order. A red
    line is superimposed indicating a $1\%$ explained variance. All components
    past the sixth contribute less than $1\%$ to the total explained variance. }
  \label{fig:pm-timeseries-pca}
\end{figure}


\section{Results}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{havok/1-havok-pm/1b__train-test-timeseries-short.pdf}
  \caption{Train-test split used for training the HAVOK model. To prevent data
    leaks, i.e. information from the future being incorporated into forecasts,
    the data was partitioned into two continuous sets to be used separately for
    model fitting and evaluation.}
  \label{fig:pm-train-test-split}
\end{figure}



\begin{table}[h]
  \caption{Results of HAVOK model hyperparameter search. Models were trained
    varying the embedding size ($N$), number of state variables $(r)$, and
    number of forcing terms ($n$). The top 10 models are presented here as
    evaluated by their RMSE and MAE values ranked in descending order.
    Additionally the top 3 models with $n=1$ are included for comparison.}
  \label{tab:havok-fit-results}
  \centering
  \begin{tabular}{ccccc} \hline
    \textbf{N} & \textbf{r} & \textbf{n} & \textbf{RMSE}  & \textbf{MAE} \\ \hline
    $30$ & $6$ & $5$ & $0.216703$ & $0.155912$ \\
    $45$ & $10$ & $5$ & $0.3649$ & $0.268842$ \\
    $30$ & $6$ & $4$ & $0.472797$ & $0.397746$ \\
    $45$ & $8$ & $5$ & $0.487448$ & $0.358938$ \\
    $30$ & $7$ & $5$ & $0.583654$ & $0.518485$ \\
    $30$ & $8$ & $5$ & $0.58551$ & $0.428986$ \\
    $30$ & $7$ & $4$ & $0.586575$ & $0.520145$ \\
    $30$ & $8$ & $4$ & $0.610296$ & $0.447673$ \\
    $30$ & $8$ & $3$ & $0.620928$ & $0.455369$ \\
    $60$ & $12$ & $5$ & $0.697797$ & $0.507499$ \\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    $105$ & $3$ & $1$ & $1.48989$ & $1.09792$  \\
    $120$ & $3$ & $1$ &  $1.58473$ & $1.1494$ \\
    $180$ & $5$ & $1$ & $1.77659$ & $1.29506$ 
  \end{tabular}
\end{table}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\columnwidth]{havok/2-havok-eval/1__predicted-ts-training.pdf}
  \caption{Time series predicted by HAVOK model with forcing functions provided.
  Inset into the figure is a subset of the }
  \label{fig:pm-havok-predictions}
\end{figure}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\columnwidth]{havok/2-havok-eval/2__A-B-Havok.pdf}
  \caption{Operators learned by the HAVOK model with $r=6$ and $n=5$. We note
    that the $\mathbf{A}$ matrix displays the expected banded diagonal
    structure. Additionally, the values of the forcing matrix $\mathbf{B}$
    decrease with each column}
  \label{fig:pm-havok-operators}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.35\columnwidth]{havok/2-havok-eval/3__A-eigvals.pdf}
  \caption{Eigenvalues of the learned HAVOK operator $\mathbf{A}$ visualized in
    the complex plane. We note that no eigenvalues had any postive real
    component reflecting the model's stability for integration over long time periods. }
  \label{fig:pm-havok-eigvals}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\columnwidth]{havok/2-havok-eval/4__forcing-statistics.pdf}
  \caption{Probability density function evaluated for the first forcing function
  compared to a zero-mean Gaussian distribution fit to the forcing data. The
  wide tails of the estimated distribution reflect the intermittent activation
  of forcing.}
  \label{fig:pm-forcing-stats}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/2-havok-eval/forcing-time-series.pdf}
  \caption{(\textbf{a}) Time series of first forcing function activation for the
    duration of the training set. (\textbf{b}) The same time series zoomed in to
    the first hours. The mean value for the forcing function was $7.94\times
    10^5$ reflecting the fact that the learned forcing functions tend to be
    centered about zero with no long-term trend.}
  \label{fig:pm-forcing-time-series}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/2-havok-eval/6__timeseries-with-forcing.pdf}
  \caption{The original PM 2.5 time series plotted together with the squared
    value of the first forcing function, $\lVert  f_1(t) \rVert^2$. By
    thresholding the values of $f_1$, the intermittent PM spikes are easily
    identified. }
  \label{fig:pm-time-series-w-forcing}
\end{figure}




\begin{table}[h]
  \caption{Evaluation of HAVOK forecasting performance as a function of
    prediction horizon from 10 seconds to 4 minutes.}
  \label{tab:havok-forecasting-results}
  \centering
  \begin{tabular}{ccccccc} \hline
    \textbf{Duration} & \textbf{RMSE}  & \textbf{RMSE} & \textbf{MAE}   & \textbf{MAE}  & \textbf{MAPE}  & \textbf{MAPE} \\
                      & \textbf{train} & \textbf{test} & \textbf{train} & \textbf{test} & \textbf{train} & \textbf{test} \\\hline
    10 sec.	  & 0.0611 & 0.0589 & 0.0415 & 0.0415 & 0.0054 & 0.0057 \\
    1 min.	  & 0.2555 & 0.2398 & 0.1757 & 0.1720 & 0.0231 & 0.0235 \\
    2 min.	  & 0.7056 & 0.6764 & 0.4874 & 0.4945 & 0.0640 & 0.0674 \\
    3 min.	  & 1.0552 & 1.0366 & 0.7290 & 0.7691 & 0.0957 & 0.1052 \\
    4 min.	  & 1.3759 & 1.3567 & 0.9530 & 1.0185 & 0.1252 & 0.1389 \\
    5 min. 	  & 1.6696 & 1.6530 & 1.1611 & 1.2475 & 0.1525 & 0.1703
  \end{tabular}
\end{table}






\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/3-forcing-fit/forecast-scatter.pdf}
  \caption{Scatter diagrams for multistep forecasts. (\textbf{a}) 10 second
    forecast. (\textbf(b)) 1 minute forecast. (\textbf{c}) 2 minute forecast.
    (\textbf{d}) 5 minute forecast. The integration time step was $\Delta t =
    10$ s so that a 5 minute forecast corresponds to a 30 step future prediction.}
  \label{fig:pm-time-series-w-forcing}
\end{figure}

