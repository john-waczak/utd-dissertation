\chapter{Modeling Local Particulate Matter Dynamics with Time Delay Embeddings}\label{ch:havok}
% \chapter{Time-delay Embedding Models for Particulate Matter - Outlier Detection and Forecasting}\label{ch:havok}

The low-cost sensor network detailed in Chapter~\ref{ch:air-network} collects a
continuous stream of air quality data for a plethora of locations with high
temporal resolution ranging between $0.1$ to $1.0$ Hz. The real-time
visualization dashboards developed for the network offer immediate insight into
local air conditions. However, the challenge remains to extract actionable
insights from the large amounts of historical data. For example, we are concerned
with answering simple questions such as: \textit{What is the typical pattern of
  PM variability at this location}, \textit{Are changes in PM gradual or due to
  identifiable events?}, and \textit{Can we use historical PM measurements to
  provide insight into future air quality trends?}.

In this chapter, we propose a physics-based model rooted in Koopman operator
theory to address these questions by accurately capturing local PM dynamics.
Specifically, we extend the \textit{Hankel Alternative View of Koopman} (HAVOK)
framework introduced by Brunton et al. to apply to time series measurements of
PM data. In this data-driven approach PM dynamics are described via a simple
linear system with aperiodic external forcing. The forcing function extracted
from the model provides a clear method to identify abrupt pollution events from
historical data. The model can be used to provide accurate short-term forecasts.
Importantly, HAVOK models require few parameters, can be fit efficiently using
established numerical linear algebra routines, and are small enough to be
readily deployed directly onto sensor hardware.


\section{Motivation}


- Pronounced seasonal variability observed for PM 2.5 in China in both urban and
rural areas around Beijing \cite{pm-patterns-china}
- Significant diurnal variation. In urban areas the distribution is bimodal with
a first peak around 7:00 - 8:00 am and an evening peak between 7:00 and 11:00 pm
corresponding to anthroprogenic activities, i.e. rush hour traffic and decrease
in boundary layer height throughout the afternoon \cite{pm-patterns-china}
- Rural diurnal pattern is unimodal with significant peak between 5 and 11 pm.

- Investigations of PM 2.5 trends in Augsburg, Germany revealed a similar
unimodal trend with peak values in the late afternoon \cite{pm-patterns-germany}

- A 2003 study in New York city anlazyed PM 2.5 measurements from 20 stations
across the city. They observed strong seasonal, weekday, and diurnal cycles with
peak concentrations between 7-9 am and minimum concentrations between 4 and 6
am. \cite{pm-patterns-nyc}
- The observed pattern suggests anthropogenic factures are primarily responsible
for observed diurnal cycle while meteorological conditions also have some influence.

- Iskandaryan et al. reviewed machine learning approaches for air quality
prediction \cite{skandaryan-2020}.
- They found that 66\% of studies considered an hourly rate
- NN and regression methods were most popular

- Prabuddha et al. combine remote sensing observations for aerosol optical depth
with meteorological data to predict ground level PM 2.5 using machine learning
together with ground based-sensors \cite{prabuddha-pm-satellite}
- Average US values exceeded 9 $\mu g/m^3$ standard 20\% of the time with the
eastern US and California exceeding the limit over 50\% of the time during the
sampling period from Jan 2020 to Jun 2023.





Particulate matter (PM) dynamics exhibit complex behavior influenced by a
variety of factors, including meteorological conditions, human activities, and
atmospheric processes.

PM concentrations typically follow well-established
diurnal patterns, with peaks often occurring during morning and evening rush
hours due to increased vehicular emissions, and lower concentrations observed
during the midday when atmospheric dispersion is more effective (Zhang et al.,
2012).

Seasonal variations are also common, with higher PM levels in winter due to
factors like heating emissions and temperature inversions that trap pollutants
close to the ground (Cheng et al., 2013).

Numerous models have been developed to analyze and forecast PM levels.

These range from statistical models, such as autoregressive integrated moving
average (ARIMA) models (Box et al., 1994), to more sophisticated machine
learning approaches like neural networks, which can capture nonlinear
relationships in the data (Zhang et al., 2020).

Physics-based models, including chemical transport models (CTMs), provide
detailed representations of atmospheric processes but require substantial
computational resources and detailed input data (Seinfeld \& Pandis, 2016).

More recently, hybrid approaches combining machine learning with physical models
have shown promise in improving forecast accuracy by leveraging the strengths of
both methodologies (Bi et al., 2021).


Physical models for particulate matter (PM) modeling and forecasting are
primarily based on the detailed representation of atmospheric processes that
govern the emission, transport, chemical transformation, and removal of PM from
the atmosphere. One of the most widely used types of physical models is the
Chemical Transport Model (CTM). These models simulate the movement of air masses
and the complex chemical reactions that occur in the atmosphere, providing a
comprehensive framework to predict PM concentrations at various spatial and
temporal scales. Examples of CTMs include models such as the Community
Multiscale Air Quality (CMAQ) model and the Weather Research and Forecasting
coupled with Chemistry (WRF-Chem) model (Byun \& Schere, 2006; Grell et al.,
2005).


CTMs rely on inputs such as emission inventories, meteorological data, and
boundary conditions to simulate the dispersion and transformation of PM. These
models are highly sophisticated and can incorporate secondary aerosol formation,
deposition processes (both dry and wet), and interactions with other pollutants
such as ozone and nitrogen oxides. However, despite their detailed nature, CTMs
can be computationally expensive and highly sensitive to uncertainties in input
data and model parameterizations (Kumar et al., 2015).


Another class of physical models used for PM forecasting is the Eulerian
grid-based models, which divide the atmosphere into a 3D grid. These models
simulate the transport and diffusion of PM across grid cells using numerical
methods to solve the advection-diffusion equations. Eulerian models like CMAQ
are known for their ability to simulate large-scale pollution events, but they
can be limited in resolving fine-scale local events, especially in urban
environments (Binkowski \& Roselle, 2003).


In addition to CTMs and Eulerian models, Lagrangian particle dispersion models
(LPDMs) are also employed for PM forecasting. LPDMs track the trajectories of
individual air parcels or particles, providing a detailed simulation of how
pollutants travel through the atmosphere. Models like the Hybrid Single-Particle
Lagrangian Integrated Trajectory (HYSPLIT) model are often used for forecasting
PM concentrations by simulating the long-range transport of pollutants and
identifying the sources of PM observed at specific locations (Stein et al.,
2015). These models are particularly useful for source apportionment and
tracking pollution plumes over long distances.

Despite their strengths, purely physical models have limitations in forecasting
the nonlinear and stochastic nature of PM concentrations, especially for
short-term predictions. To overcome these challenges, hybrid models that combine
physical models with data-driven approaches (such as machine learning) have
gained popularity in recent years (Bi et al., 2021). These hybrid models aim to
leverage the process-based understanding of physical models with the pattern
recognition capabilities of statistical or machine learning models, improving
forecast accuracy and computational efficiency.




- Known dynamics of particulate matter, e.g. the diurnal cycle

- Physical models for particulate matter dynamics, e.g. simulation and modeling
approaches


- Methods for PM forecasting


- Cite the prabuddha paper for remote sensing inference

- Eulerian models: simulate a volumes of air (in some kind of lattice)

- Combine relevant atmospheric chemistry and meteorological processes --> CMAQ

- Lagrangian models: simulate individual particle trajectories

- Combine Eulerian and Lagrangian models into hybrid system to model both
transport (advection) and diffusion of plumes --> HYSPLIT

- Time series methods?

- ARIMA, NN approachs, etc...

- Dynamic Mode Decomposition and Koopman operator theory for time series analysis

- The HAVOK method... NOTE that it is promising, but to the best of our current
knowledge, there has not been any application of HAVOK to \textit{real}, noisy data.


% Particulate matter (PM) dynamics exhibit complex behavior influenced by a variety of factors, including meteorological conditions, human activities, and atmospheric processes. PM concentrations typically follow well-established diurnal patterns, with peaks often occurring during morning and evening rush hours due to increased vehicular emissions, and lower concentrations observed during the midday when atmospheric dispersion is more effective (Zhang et al., 2012). Seasonal variations are also common, with higher PM levels in winter due to factors like heating emissions and temperature inversions that trap pollutants close to the ground (Cheng et al., 2013). Numerous models have been developed to analyze and forecast PM levels. These range from statistical models, such as autoregressive integrated moving average (ARIMA) models (Box et al., 1994), to more sophisticated machine learning approaches like neural networks, which can capture nonlinear relationships in the data (Zhang et al., 2020). Physics-based models, including chemical transport models (CTMs), provide detailed representations of atmospheric processes but require substantial computational resources and detailed input data (Seinfeld & Pandis, 2016). More recently, hybrid approaches combining machine learning with physical models have shown promise in improving forecast accuracy by leveraging the strengths of both methodologies (Bi et al., 2021). 

% ---



- Dynamics of particulate matter


- Time series models for PM (how good are forecasts)


- Value of a physics-based models, e.g. chemical transport

- Limitations of these models are due to poorly resolved meteorological
obsevations, for example, high resolution ECMWF analysis are at 0.1 x 0.1 km
grids with the lowest vertical model layer hundreds of meters above the ground.

- Many of these are being developed but have yet to see application to
realistic, noisy datasets




It should be noted that
despite the rapid pace of development in the fields of data-driven and
scientific machine learning, many recently developed techniques like the
Universal Differential Equations (UDEs), Hamiltonian Neural Networks, and others
have yet to see wide spread application on noisy, real-world datasets. Our
secondary goal for this chapter is therefore to demonstrate how, with some
slight modifications, these techniques can be applied to real-world problems.


In order to provide actionable insights we must be able to effectively model the
dynamics of our collected time series. In a perfect world, we would measure all
relevant physical quantities such that the time evolution of local air quality
at each sensor could be obtained by simulating the relevant micro-physics.
However, low cost sensor networks are not equipped with all the necessary
reference grade instruments needed to perform such simulations; accurate winds
speed and direction sensors alone can cost hundreds to thousands of dollars and
remote sensing data products are often unreliable at the ground level (i.e. in
the human head space). We therefore are motivated to develop techniques to model
our collected time series using only the data provided at a single node. There
are many approaches to this task in the statistics and machine learning
literature including statistical models like ARIMA and deep learning methods
like Recurrent Neural Networks \cite{intro-to-time-series-models,
  time-series-rnns}. While these methods can often lead to robust short term
predictions, they do not incorporate prior physics knowledge and therefore are
not primed to take advantage of underlying dynamical laws. Recently two
interesting physics-informed, data driven techniques have been developed for
just this type of scenario. The first we shall examine is the so-called
\textit{Hankel Alternative View Of Koopman} (HAVOK) framework which extends the
principle of dynamic mode decomposition to nonlinear systems
\cite{brunton-havok}. The second is an technique dubbed the \textit{Hamiltonian
  Neural Network} which extends the notion of a Neural Ordinary Differential
Equation to allow a neural network to learn coordinate transformations or the
original time series data which satisfy \textit{Hamiltons equations}
\cite{greydanus-hnn}.




Note the additional constraint that our model should be both physically
interpretable and small enough to be easily trained and deployed at scale. While
complicated DNN based approaches such as deep recurrent networks may provide an
ability to forecast, the size and training times involved for these approaches
are prohibitively high if we wish to train and deploy these models directly to
sensors in the network in an automated fashion. Also note the other HAVOK paper
which attempts to use HAVOK for predictions which showed that the HAVOK model
provides a superior one step prediction compared to other state of the art
models.


\section{Hankel Alternative View of Koopman}


- Koopman operator theory for dynamical systems \cite{brunton-koopman-theory}
- Original HAVOK paper \cite{brunton-havok-orig}.
- HAVOK paper w/ ML for forcing \cite{havok-ml}
- sHAVOK and connections to frenet-serret frame \cite{havok-diffgeo}


\subsection{Global Motivation: Koopman Operator Theory}

\subsection{Time-Delay Embeddings and Taken's Theorem}
\subsection{Hankel Alternative View of Koopman}
\subsection{Local Motivation: Frenet-Serret Frame}
\subsection{Structured HAVOK}
\subsection{Simple Example: Lorenz System}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/2__lorenz-orig.png}
  \caption{(\textbf{a}) Time series for the $x$, $y$, and $z$ components of the
    Lorenz system. (\textbf{b}) 3-dimensional visualization of the attractor
    formed from $x$, $y$, and $z$ components. We note that there are two clear
    lobes corresponding to the two attractors of the system.}
  \label{fig:lorenz-time-series-orig}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\columnwidth]{havok/0-havok-lorenz/1b_A-B-sHAVOK.pdf}
  \caption{Fitted operators $\mathbf{A}$ and $\mathbf{B}$ for the Lorenz system
    learned by the HAVOK model corresponding to the linear dynamics and
    intermittent forcing.}
  \label{fig:lorenz-A-B-heatmap}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/3__havok-embedding.pdf}
  \caption{(\textbf{a}) Time series for first 3 components $v\_i$ of the HAVOK
    model. Original time series for each component are shown in blue. Time
    series predicted by the HAVOK model are overlaid in red. (\textbf(b)) The
    attractor formed by the first 3 embedding coordinates. By Taken's theorem,
    this attractor is diffeomorphic to the original attractor.}
  \label{fig:lorenz-havok-embedding}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\columnwidth]{havok/0-havok-lorenz/6__forcing-statistics.pdf}
  \caption{Proabability density function for forcing time series learned by the
    HAVOK model. The PDF is compared to a Gaussian fit for the same data. The
    wide tails of the PDF reflect that the forcing is intermittent.}
  \label{fig:lorenz-forcing-stats}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/0-havok-lorenz/7__attractor-w-forcing.pdf}
  \caption{(\textbf{a}) The original time series $x(t)$ plotted with the squared
  forcing time series $v_r(t)$. Red colors indicate forcing above a chosen
  threshold which accurately identify lobe-switching events. (\textbf{b}) The
  Lorenz attractor colored using the same scheme.}
  \label{fig:lorenz-attractor-forcing}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\columnwidth]{havok/0-havok-lorenz/9__timeseries-reconstruction.pdf}
  \caption{The reconstructed time series for $x(t)$ using the learned HAVOK model.}
  \label{fig:lorenz-reconstruction}
\end{figure}




\section{Study Overview}

- Describe dataset, e.g. the specific central node, focus on PM 2.5, and the
collection period (find continuous data during no-rain period during the summer
of 2023 to evaluate HAVOK model independent of problems introduced by
hygroscopic growth)


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{havok/1-havok-pm/1__timeseries-full-short.pdf}
  \caption{Time series of PM $2.5$ measurements from Central Hub 4 located in
    Planeo, Tx starting on 2023-08-04. This time series was selected during the
    long period of no precipitation during 2023 in order to limit the impact of
    hygroscopic growth on observed values. Note the occasional, thin vertical
    spikes corresponding to acute pollution events.}
  \label{fig:pm-timeseries-orig}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\columnwidth]{havok/1-havok-pm/1__pca-explained-variance.pdf}
  \caption{Explained variance of components from a principal component
    decomposition of the PM 2.5 time series sorted in decreasing order. A red
    line is superimposed indicating a $1\%$ explained variance. All components
    past the sixth contribute less than $1\%$ to the total explained variance. }
  \label{fig:pm-timeseries-pca}
\end{figure}


\section{Results}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\columnwidth]{havok/1-havok-pm/1b__train-test-timeseries-short.pdf}
  \caption{Train-test split used for training the HAVOK model. To prevent data
    leaks, i.e. information from the future being incorporated into forecasts,
    the data was partitioned into two continuous sets to be used separately for
    model fitting and evaluation.}
  \label{fig:pm-train-test-split}
\end{figure}



\begin{table}[h]
  \caption{Results of HAVOK model hyperparameter search. Models were trained
    varying the embedding size ($N$), number of state variables $(r)$, and
    number of forcing terms ($n$). The top 10 models are presented here as
    evaluated by their RMSE and MAE values ranked in descending order.
    Additionally the top 3 models with $n=1$ are included for comparison.}
  \label{tab:havok-fit-results}
  \centering
  \begin{tabular}{ccccc} \hline
    \textbf{N} & \textbf{r} & \textbf{n} & \textbf{RMSE}  & \textbf{MAE} \\ \hline
    $30$ & $6$ & $5$ & $0.216703$ & $0.155912$ \\
    $45$ & $10$ & $5$ & $0.3649$ & $0.268842$ \\
    $30$ & $6$ & $4$ & $0.472797$ & $0.397746$ \\
    $45$ & $8$ & $5$ & $0.487448$ & $0.358938$ \\
    $30$ & $7$ & $5$ & $0.583654$ & $0.518485$ \\
    $30$ & $8$ & $5$ & $0.58551$ & $0.428986$ \\
    $30$ & $7$ & $4$ & $0.586575$ & $0.520145$ \\
    $30$ & $8$ & $4$ & $0.610296$ & $0.447673$ \\
    $30$ & $8$ & $3$ & $0.620928$ & $0.455369$ \\
    $60$ & $12$ & $5$ & $0.697797$ & $0.507499$ \\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
    $105$ & $3$ & $1$ & $1.48989$ & $1.09792$  \\
    $120$ & $3$ & $1$ &  $1.58473$ & $1.1494$ \\
    $180$ & $5$ & $1$ & $1.77659$ & $1.29506$ 
  \end{tabular}
\end{table}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\columnwidth]{havok/2-havok-eval/1__predicted-ts-training.pdf}
  \caption{Time series predicted by HAVOK model with forcing functions provided.
  Inset into the figure is a subset of the }
  \label{fig:pm-havok-predictions}
\end{figure}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\columnwidth]{havok/2-havok-eval/2__A-B-Havok.pdf}
  \caption{Operators learned by the HAVOK model with $r=6$ and $n=5$. We note
    that the $\mathbf{A}$ matrix displays the expected banded diagonal
    structure. Additionally, the values of the forcing matrix $\mathbf{B}$
    decrease with each column}
  \label{fig:pm-havok-operators}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.35\columnwidth]{havok/2-havok-eval/3__A-eigvals.pdf}
  \caption{Eigenvalues of the learned HAVOK operator $\mathbf{A}$ visualized in
    the complex plane. We note that no eigenvalues had any postive real
    component reflecting the model's stability for integration over long time periods. }
  \label{fig:pm-havok-eigvals}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\columnwidth]{havok/2-havok-eval/4__forcing-statistics.pdf}
  \caption{Probability density function evaluated for the first forcing function
  compared to a zero-mean Gaussian distribution fit to the forcing data. The
  wide tails of the estimated distribution reflect the intermittent activation
  of forcing.}
  \label{fig:pm-forcing-stats}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/2-havok-eval/forcing-time-series.pdf}
  \caption{(\textbf{a}) Time series of first forcing function activation for the
    duration of the training set. (\textbf{b}) The same time series zoomed in to
    the first hours. The mean value for the forcing function was $7.94\times
    10^5$ reflecting the fact that the learned forcing functions tend to be
    centered about zero with no long-term trend.}
  \label{fig:pm-forcing-time-series}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/2-havok-eval/6__timeseries-with-forcing.pdf}
  \caption{The original PM 2.5 time series plotted together with the squared
    value of the first forcing function, $\lVert  f_1(t) \rVert^2$. By
    thresholding the values of $f_1$, the intermittent PM spikes are easily
    identified. }
  \label{fig:pm-time-series-w-forcing}
\end{figure}




\begin{table}[h]
  \caption{Evaluation of HAVOK forecasting performance as a function of
    prediction horizon from 10 seconds to 4 minutes.}
  \label{tab:havok-forecasting-results}
  \centering
  \begin{tabular}{ccccccc} \hline
    \textbf{Duration} & \textbf{RMSE}  & \textbf{RMSE} & \textbf{MAE}   & \textbf{MAE}  & \textbf{MAPE}  & \textbf{MAPE} \\
                      & \textbf{train} & \textbf{test} & \textbf{train} & \textbf{test} & \textbf{train} & \textbf{test} \\\hline
    10 sec.	  & 0.0611 & 0.0589 & 0.0415 & 0.0415 & 0.0054 & 0.0057 \\
    1 min.	  & 0.2555 & 0.2398 & 0.1757 & 0.1720 & 0.0231 & 0.0235 \\
    2 min.	  & 0.7056 & 0.6764 & 0.4874 & 0.4945 & 0.0640 & 0.0674 \\
    3 min.	  & 1.0552 & 1.0366 & 0.7290 & 0.7691 & 0.0957 & 0.1052 \\
    4 min.	  & 1.3759 & 1.3567 & 0.9530 & 1.0185 & 0.1252 & 0.1389 \\
    5 min. 	  & 1.6696 & 1.6530 & 1.1611 & 1.2475 & 0.1525 & 0.1703
  \end{tabular}
\end{table}






\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{havok/3-forcing-fit/forecast-scatter.pdf}
  \caption{Scatter diagrams for multistep forecasts. (\textbf{a}) 10 second
    forecast. (\textbf(b)) 1 minute forecast. (\textbf{c}) 2 minute forecast.
    (\textbf{d}) 5 minute forecast. The integration time step was $\Delta t =
    10$ s so that a 5 minute forecast corresponds to a 30 step future prediction.}
  \label{fig:pm-time-series-w-forcing}
\end{figure}

