## Data Assimilation

<!-- Add some commands to make life easier -->
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}



**NOTE**: It would be nice to provide additional derivations (where possible) in a Bayesian framework... We should also liberally cite Dr. Lary's original papers on the chemical 4d-var implementation. 

### Overview

The proper application of scientific models to make real-world predictions requires that we commit ourselves to a full accounting of all possible sources of uncertainty when reporting results. Further, the explosion of *big data* across scientific fields provieds a plethora observational data that our models are typically unequipped to incorporate when making predictions. The field of **Data Assimilation** addresses this problem by providing a family of techniques engineered to combine model output together with observational data whilst enabling a complete accounting the sources of uncertainty. For chaotic systems in particular, data assimilation enables integration on long time scales that would be impossible via models alone. 


In this overview, we will follow the examples from [this nice paper](https://www.mdpi.com/2311-5521/5/4/225/htm). 

### Framing the Problem
Data assimilation can be understood most generally in terms of dyscrete dynamical systems. This enables us to apply the methods to most mathematical models from gridded PDE solvers to systems of ordinary differential equations. Our goal is to find the best prediction for the system state vector $u$ that combines our model predictions, also known as forecasts, with observational data. Model predictions are summarized via the discrete update equation: 

\begin{equation}
    u_{k+1} = \mathcal{M}(u_k; \theta)
\end{equation}

For ODE systems, $\mathcal{M}$ represents the time integration scheme for a system of ODEs like 

\begin{equation}
    \dfrac{du}{dt} = f(u, t; \theta)
\end{equation}


To measure the performance of our assimilation scheme, we denote the *true* value of the state vector as $u^{(t)}$. The output of our model is denoted $u^{(b)}$ (*b* subscript for *background*). The discrepancy between the true value and our forecast is denoted $\xi^{(b)} = u^{(t)} - u^{(b)}$ characterizing the extent to which our model prediction is imperfect. 

The observations of our system are denoted by $w_k = w(t_k)$. These observations do not necessarily need to be components of the state vector $u$, but rather, are related to it via the *observation function* $h$. For example, one may attempt to predict sea surface temperature using data assimilation with data from satellite observations. The function $h$ would then be the Stefan-Boltzmann law. However, real world data is noisy, which we must take into account. We write 

\begin{equation}
    w_k = h(u_k) + \xi_k^{(m)}
\end{equation}

where $\xi_k^{(m)}$ denotes this measurement noise. 


Given our model predictions $u_{k}^{(b)}$ and observations $w_k$, we seek to obtain the *optimal* or best-possible prediction called the **analysis**, $u^{(a)}$. This analysis will still not be perfect, so we further specify the analysis error via 

\begin{equation}
\xi^{(a)} = u^{(t)} - u^{(a)}
\end{equation}


### Summary 

\begin{align}
    &u_k^{(t)} \in \mathbb{R}^n &\text{the true state vector} \\ 
    &u_k^{(b)} \in \mathbb{R}^n &\text{the kth model forecast} \\ 
    &u_k^{(a)} \in \mathbb{R}^n &\text{the analysis} \\ 
    &w_k \in \mathbb{R}^m &\text{the kth observation vector} \\ 
    &\xi^{(b)} \in \mathbb{R}^n &\text{the model forecast error}\\
    &\xi^{(m)} \in \mathbb{R}^m &\text{the observation noise vector}\\ 
    &\xi^{(a)} \in \mathbb{R}^n &\text{the analysis error}\\
    &\xi^{(p)} \in \mathbb{R}^n &\text{the process noise if we used our model on the true state}\\
    &\mathcal{M}:\mathbb{R}^n\to\mathbb{R}^n &\text{the model update function}\\
    &f:\mathbb{R}^n\to\mathbb{R}^n &\text{differential equation model}\\ 
    &h:\mathbb{R}^n\to\mathbb{R}^m  &\text{observation function}
\end{align}


### Assumptions
To make possible the derivation of a *unique* analysis $u^{(a)}$, the following assumptions are in order.

\begin{align}
    &\E[\xi_k^{(b)}] = 0 & &\E[\xi_k^{(b)}(\xi_j^{(b)})^T] = 0 \text{ for } k\neq j\\
    &\E[\xi_k^{(m)}] = 0 & &\E[\xi_k^{(m)}(\xi_j^{(m)})^T] = 0 \text{ for } k\neq j\\
    &\E[\xi_k^{(b)}(u_0)^T] = 0 & &\E[\xi_k^{(m)}(u_0)^T] = 0\\
    &\E[\xi_k^{(b)}\xi_j^{(m)}] = 0 & &  \\ 
    &\E[u_k^{(t)}] = u_k^{(b)} & &
\end{align}

We also define the error covariance matrices

\begin{align}
    Q_k &:= \E[\xi_k^{(p)}(\xi_k^{(p)})^T] \\ 
    R_k &:= \E[\xi_k^{(m)}(\xi_k^{(m)})^T] \\ 
    B_k &:= \E[\xi_k^{(b)}(\xi_k^{(b)})^T] 
\end{align}

which we will use in our consideration of the final error of our analysis.



### Kalman Filtering 
Given some model for the error covariance matrices $Q_k$ and $R_k$, we would like a method that propagates *both* our model **and** the errors forward. This way we may guarantee that the accuracy of our analysis doesn't come at the cost of higher uncertainty. 

The original implementation of the Kalman filter was for strictly linear systems. We will first develop the analysis for this simplified case adn then will generalize to the **Extended Kalman Filter** (EKF) that can handle fully nonlinear situations.

In the linear case, our system may be written as 

\begin{align}
    u_{k+1}^{(t)} &= M_ku_k^{(t)} + \xi_{k+1}^{(p)} \\ 
    w_k &= H_ku_k^{(t)} + \xi_k^{(m)}
\end{align}

where $M_k$ and $H_k$ are now matrices defining the linear problem. 

The goal of the Kalman filter is to derive the analysis $u^{(a)}$ which optimizes the trace of the analysis error covariance matrix (i.e. sum of squared errors): 

\begin{equation}
    \mathrm{Tr}\left( P_k\right) := \E[(u_k^{(t)}-u_k^{(a)})^T(u_k^{(t)}-u_k^{(a)})]
\end{equation}

Finding the analysis consists of two steps: the forecast step and the assimilation step.

#### Forecast Step
Assume we have the analysis at time $t_k$ denoted $u_k^{(a)}$. Then the forecast for time $t_{k+1}$ is

\begin{equation}
    u_{k+1}^{(b)} = M_ku_k^{(a)}
\end{equation}

The background error is therefore 

\begin{align}
    \xi_{k+1}^{(b)} &= u_{k+1}^{(t)} - u_{k+1}^{(b)} \\ 
    &= M_ku_k^{(t)}+\xi_{k+1}^{(p)} - M_{k}u_k^{(a)} \\ 
    &= M_k\left(u_k^{(t)}-u_k^{(a)} \right) + \xi_{k+1}^{(p)} \\ 
    &= M_k\xi_k^{(a)} + \xi_{k+1}^{(p)}
\end{align}

We may now evaluate the covariance matrix of our background estimate as: 

\begin{align}
    B_{k+1} &= \E[\xi_{k+1}^{(b)}(\xi_{k+1}^{(b)})^T] \\ 
    &= \E\left[\left(M_k\xi_k^{(a)} + \xi_{k+1}^p \right) \left(M_k\xi_k^{(a)} + \xi_{k+1}^p \right)^T \right] \\ 
\end{align}

If we presume that $\E[\xi_k^{(b)}(\xi_{k+1}^{(p)})^T] = 0$, then the cross terms vanish and we are left with 

\begin{equation}
    \boxed{B_{k+1} = M_kP_kM_k^T + Q_{k+1}}
\end{equation}

Thus we now have the background (i.e forecast) estimate of the state at $t_{k+1}$ and its covariance matrix. Given a measurement $w_{k+1}$ at the same time with covariance matrix $R_{k+1}$, then we may now perform the assimilation step where we fuse the two sources of information to obtain $u_{k+1}^{(a)}$ and $P_{k+1}$.

#### Data Assimilation Step
Let's suppose that the analysis has the form 

\begin{equation}
    u_{k+1}^{(a)} = \nu + K_{k+1}w_{k+1}
\end{equation}

for some vector $\nu\in\R^n$ and matrix $K_{k+1}\in\R^{m\times n}$. In a perfect world, we would have $\E[u_{k}^{(t)}-u_{k}^{(a)}] = 0$. Therefore, 

\begin{align}
    0 &= \E[u_k^{(t)} - u_k^{(a)}] \\ 
    &= \E[(u_k^{(b)} + \xi_k^{(b)}) - (\nu + K_kw_k)] \\ 
    &= \E[(u_k^{(b)} + \xi_k^{(b)}) - (\nu + K_kH_ku_k^{(t)} + K_k\xi_k^{(m)})] \\ 
    &= \E[u_k^{(b)}] + \E[\xi_k^{(b)}] - \E[\nu] -K_kH_k\E[u_k^{(t)}] - K_k\E[\xi_k^{(m)}]\\
    &= u_k^{(b)} + 0 - \nu - K_kH_ku_k^{(b)} - 0 \\ 
    &= u_k^{(b)} - \nu - K_kH_ku_k^{(b)} \\ 
    \Rightarrow \nu &= u_k^{(b)} - K_kH_ku_k^{(b)}
\end{align}

which we now substitute to obtain 

\begin{equation}
    \boxed{u_k^{(a)} = u_k^{(b)} + K_k(w_k - H_ku_k^{(b)})}
\end{equation}

Now that we know the form for the analysis we may derive the optimal matrix $K_k$ by optimization of $P_k$. We have

\begin{align}
	\xi_k^{(a)} &= u_k^{(t)} - u_k^{(a)} \\ 
                &= M_{k-1}u_{k-1}^{(t)} + \xi_{k}^{(p)} - u_k^{(b)} - K_k\left(w_k - H_ku_k^{(b)} \right) \\ 
                &= M_{k-1}u_{k-1}^{(t)} + \xi_{k}^{(p)} - M_{k-1}u_{k-1}^{(a)} - K_k\left(H_ku_k^{(t)} + \xi_k^{(m)} - H_ku_k^{(b)} \right) \\ 
                &= M_{k-1}u_{k-1}^{(t)} + \xi_{k}^{(p)} - M_{k-1}u_{k-1}^{(a)} - K_kH_ku_k^{(t)} - K_k\xi_k^{(m)} + K_kH_ku_k^{(b)} \\ 
                &= M_{k-1}u_{k-1}^{(t)} + \xi_{k}^{(p)} - M_{k-1}u_{k-1}^{(a)} - K_kH_ku_k^{(t)} - K_k\xi_k^{(m)} + K_kH_ku_k^{(b)} \\ 
                &= \Big\{ M_{k-1}(\xi_{k-1}^{(a)}+u_{k-1}^{(a)}) + \xi_{k}^{(p)} - M_{k-1}u_{k-1}^{(a)} \Big\} - K_kH_ku_k^{(t)} - K_k\xi_k^{(m)} + K_kH_ku_k^{(b)} \\ 
                &= \Big\{ M_{k-1}\xi_{k-1}^{(a)} + \xi_{k}^{(p)} \Big\} - K_kH_ku_k^{(t)} + K_kH_ku_k^{(b)} - K_k\xi_k^{(m)}\\ 
                &= M_{k-1}\xi_{k-1}^{(a)} + \xi_{k}^{(p)} - K_kH_k(M_{k-1}u_{k-1}^{(t)} + \xi_k^{(b)}) + K_kH_ku_k^{(b)} - K_k\xi_k^{(m)}\\ 
                &= M_{k-1}\xi_{k-1}^{(a)} + \xi_{k}^{(p)} - K_kH_kM_{k-1}(\xi_{k-1}^{(a)} + u_{k-1}^a) - K_kH_k\xi_k^{(b)} + K_kH_ku_k^{(b)} - K_k\xi_k^{(m)}\\ 
                &= M_{k-1}\xi_{k-1}^{(a)} + \xi_{k}^{(p)} - K_kH_kM_{k-1}(\xi_{k-1}^{(a)} + u_{k-1}^a) - K_kH_k\xi_k^{(b)} + K_kH_kM_{k-1}u_{k-1}^{(a)} - K_k\xi_k^{(m)}\\ 
                &= M_{k-1}\xi_{k-1}^{(a)} + \xi_{k}^{(p)} - K_kH_kM_{k-1}\xi_{k-1}^{(a)} - K_kH_k\xi_k^{(b)} - K_k\xi_k^{(m)}\\ 
                &= \big(I-K_kH_k \big)(M_{k-1}\xi_{k-1}^{(a)} - \xi_{k}^p) - K_k\xi_k^{(m)}\\
\end{align}

and therefore the covariance matrix is 

\begin{align}
    P_k &= \E[\xi_{k}^{(a)}(\xi_{k}^{(a)})^T] \\ 
        &= \E\Big[\left(\big(I-K_kH_k \big)(M_{k-1}\xi_{k-1}^{(a)} - \xi_{k}^p) - K_k\xi_k^{(m)} \right) \left(\big(I-K_kH_k \big)(M_{k-1}\xi_{k-1}^{(a)} - \xi_{k}^p) - K_k\xi_k^{(m)} \right)^T \Big] \\ 
        &= \big(I-K_kH_k \big)M_{k-1}\E[\xi_{k-1}^{(a)}(\xi_{k-1}^{(a)})^T]M_{k-1}^T\big(I-K_kH_k \big)^T + \big(I-K_kH_k \big)\E[\xi_k^{(p)}(\xi_k^{(p)})^T]\big(I-K_kH_k \big)^T \\ 
        & \qquad \qquad - K_k\E[\xi_{k}^{(m)}(\xi_k^{(m)})^T]K_k^T \\ 
        &= \big(I-K_kH_k \big)B_k\big(I-K_kH_k \big)^T - K_kR_kK_k^T
\end{align}

####  Deriving $K_k$
The Kalman filter is defined at that $K_k$ which which minimizes the sum of squared analysis errors, i.e. the trace of the analysis error covariance matrix. The following identies will be useful: 

\begin{align}
    \mathop{\nabla}_{A}\text{tr}(AB) &= B^T \\ 
    \mathop{\nabla}_{A}\text{tr}(BA^T) &= B \\ 
    \mathop{\nabla}_{A}\text{tr}(ABA^T) &= AB^T + AB  \\ 
\end{align}

from which we obtain 

\begin{align}
    0 &= \mathop{\nabla}_{K_k}\text{tr}(P_k) \\ 
      &= \mathop{\nabla}_{K_k}\Big\{ B_k -B_kH_k^TK_k^T - K_kH_kB_k  + K_kH_kB_kH_k^TB_k^T - K_kR_kK_k^T \Big\} \\ 
      &= -B_kH_k^T - (H_kB_k)^T + K_k\left[H_kB_kH_k^T + (H_kB_kH_k^T)^T - R_k+R_k^T\right] \\
      &= -2B_kH_k^T + 2K_k\left(H_kB_kH_k^2 - R_k \right) \\ 
  \Rightarrow K_k &= B_kH_k^T\Big[ H_kB_kH_k^T - R_k \Big]^{-1}
\end{align}

we now substitute this result to obtain a simplified form for $P_k$. 

\begin{align}
    P_k &= \left(I - K_kH_k \right)B_k\left(I - K_kH_k \right)^T + K_kR_kK_k^T \\ 
        &= \left(I - K_kH_k \right)B_k - \left(I - K_kH_k \right)B_k\left(K_kH_k \right)^T + K_kR_kK_k^T \\
        &= \left(I - K_kH_k \right)B_k -\left\{ \left(I - K_kH_k \right)B_k\left(K_kH_k \right)^T + K_kR_kK_k^T \right\} \\
        &= \left(I - K_kH_k \right)B_k -\left\{ \left(I - K_kH_k \right)B_kH_k^TK_k^T + K_kR_kK_k^T \right\} \\
        &= \left(I - K_kH_k \right)B_k -\left\{ \left(I - K_kH_k \right)B_kH_k^T + K_kR_k \right\}K_k^T \\
        &= \left(I - K_kH_k \right)B_k -\left\{ B_kH_k^T - K_k\left( H_kB_kH_k^T + R_k \right)  \right\}K_k^T \\
        &= \left(I - K_kH_k \right)B_k -\left\{ B_kH_k^T - B_kH_k^T \right\}K_k^T \\
        &= \left(I - K_kH_k \right)B_k 
\end{align}

**NOTE**: we have used the fact that covariance matrices are symmetric. 

#### Summary 
Let's summarize the whole process. We have 

1. Initialization 
We must set the system to some initial condition. This means we must define $u_0^a$ and $P_0$. We must also come up with a model for the process noise covariance $Q_k$ and measurement error covariance $R_k$.

2. Forecast Step 

\begin{align}
    u_k^{(b)} &= M_{k-1}u_{k-1}^{(a)} \\ 
    B_k &= M_{k-1}P_{k-1}M_{k-1}^T + Q_{k}
\end{align}

3. Assimilation Step 

\begin{align}
    K_k &= B_kH_k^T\Big[ H_kB_kH_k^T - R_k \Big]^{-1}  \\ 
    u_k^{(a)} &= u_k^{(b)} + K_k(w_k - H_ku_k^{(b)})\\ 
    P_k &= \left(I - K_kH_k \right)B_k 
\end{align}


### Extended Kalman Filter

Given the nonlinear nature of many scientific models it is desirable to extend the **Kalman Filter** to be able to handle nonlinear models $f(\cdot)$ (and by extension, their update function $\mathcal{M}(\cdot)$), and nonlinear observation functions $h(\cdot)$. This can be accomplished so long as these functions are sufficiently smooth ($C^1$ to be precise) so as to admit valid Taylor approximations to first order. That is, 

\begin{align}
    \mathcal{M}(u_{k}) &\approx \mathcal{M}(u_k^{(a)}) + D_{M}(u_k^{(a)})\xi_k^{(a)} & h(u_k) &\approx h(u_k^{(b)}) + D_h(u_k^{(b)})\xi_k^{(b)} \\ 
    D_{M} &:= \left[\dfrac{\partial \mathcal{M}_i}{\partial u_j} \right] & D_h &:= \left[ \dfrac{\partial h_i}{\partial u_j}\right]
\end{align}

where $\mathcal{M}_i$ and $h_i$  denote the ith component functions of $\mathcal{M}$ and $h$. 


Using these substitutions for the previously linear functions $M_k$ and $H_k$, we may follow the same derivation to obtain the following procedure. 

1. Initialization 
To begin we must choose values for $u_0^{(a)}$ and $P_0$. We must also provide models for $Q_k$ and $R_k$. 

2. Forecast Step 

\begin{align}
    u_k^{(b)} &= \mathcal{M}(u_{k-1}^{(a)}) \\ 
    B_k &= D_M(u_{k-1}^{(a)})P_{k-1}D_M^T(u_{k-1}^{(a)}) + Q_k
\end{align}

3. Assimilation Step 

\begin{align}
    K_k &= B_kD_M^T(u_k^{(b)})\left[ D_h(u_k^{(b)})B_kD_h^T(u_k^{(b)}) + R_k \right]^{-1}\\
    u_k^{(a)} &= u_k^{(b)} + K_k(w_k - h(u_k^{(b)})) \\ 
    P_k &= \left( I - K_kD_h(u_k^{(b)}) \right)B_k
\end{align}




### 3D-Var 
For the **Kalman Filter** and the **EKF**, we derived the optimal way to combine observation with simulation so as to minimize the trace of the analysis error covariance matrix, $P_k$. An alternative approach is to recast the problem as a pure optimzation problem where rather than finding a filter $K_k$ that will add an innovation to $u_k^{(b)}$ to obtain the analysis $u_k^{(a)}$, we obtain the analysis by optimizing the following cost function

\begin{equation}
J(u) = \frac{1}{2}\left(w - h(u) \right)^TR^{-1}\left(w - h(u) \right) + \frac{1}{2}\left(u - u^{(b)} \right)^TB^{-1}\frac{1}{2}\left(u - u^{(b)} \right)
\end{equation}

which we can justify as coming from the joint probability distribution assuming Gaussian errors

\begin{equation}
\mathcal{P}(u|w) = C\exp\left(- \frac{1}{2}\left(u - u^{(b)} \right)^TB^{-1}\frac{1}{2}\left(u - u^{(b)} \right) \right)\cdot\exp\left(-  \frac{1}{2}\left(w - h(u) \right)^TR^{-1}\left(w - h(u) \right) \right)
\end{equation}

with model error covariance $B$ and measurement error covariance $R$ as before. This is clearly a *very strong assumption*.

To optimize $J(u)$, we begin by taking it's gradient.

\begin{equation}
    \nabla_uJ(u) = -D_h^TR^{-1}(w-h(u)) + B^{-1}(u-u^{(b)})
\end{equation}

Thus, finding the analysis $u^{(a)}$ ammounts to solving the system 

\begin{equation}
    a-D_h^TR^{-1}(w-h(u^{(a)})) + B^{-1}(u^{(a)}-u^{(b)}) = 0
\end{equation}

As for Kalman filtering, let's begin with the assumption that our model and observation function are linear. 

#### Linear Case
Suppose that we have $h(u) = Hu$ so that $D_h(u) = H$. Then, we have 


\begin{align}
    D_h^TR^{-1}(w-Hu^{(a)}) &= B^{-1}(u^{(a)}-u^{(b)}) \\ 
    D_h^TR^{-1}w - D_h^TR^{-1}Hu^{(a)} &= B^{-1}u^{(a)} - B^{-1}u^{(b)} \\ 
    \left(D_h^TR^{-1}H + B^{-1} \right)u^{(a)} &= D_h^TR^{-1} + B^{-1}u^{(b)} \\
    \left(H^TR^{-1}H + B^{-1} \right)u^{(a)} &= H^TR^{-1} + B^{-1}u^{(b)}
\end{align}

Thus we see that the analysis is given by 

\begin{equation}
    u^{(a)} = u^{(b)} + BH^T\left( R + HB^TH \right)^{-1}(w-Hu^{(b)})
\end{equation}

which agrees with what we found for the Linear Kalman Filter. 

#### Nonlinear Case

To deal with the nonlinearity, we can expand $h$ about an initial guess $u^{(c)}$ which we will later choose to be $u^{(b)}$ for convenience. 

\begin{equation}
    h(u^{(a)}) \approx h(u^{(c)}) + D_h(u^{(c)})\Delta u
\end{equation}

Using this, we have 

\begin{align}
    D_h^T(u^{(a)})R^{-1}(w-h(u^{(a)})) &= B^{-1}(u^{(a)} - u^{(b)}) \\ 
    D_h^T(u^{(a)})R^{-1}(w-h(u^{(c)})-D_h(u^{(c)})\Delta u) &\approx B^{-1}(u^{(c)} + \Delta u - u^{(b)}) \\ 
    D_h^T(u^{(c)})R^{-1}(w-h(u^{(c)})-D_h(u^{(c)})\Delta u) &\approx B^{-1}(u^{(c)} + \Delta u - u^{(b)}) 
\end{align}

which we now solve for the update $\Delta u$ to obtain the linear system 

\begin{equation}
    \left(B^{-1} + D_h^T(u^{(c)})R^{-1}D_h(u^{(c)}) \right)\Delta u = B^{-1}(u^{(b)}-u^{(c)}) + D_h^T(u^{(c)})R^{-1}(w-h(u^{(c)}))
\end{equation}

Thus we have the following prescription 
1. To begin, take $u^{(c)} == u^{(b)}$. 
2. Solve the system 

\begin{equation}
    \left(B^{-1} + D_h^T(u^{(c)})R^{-1}D_h(u^{(c)}) \right)\Delta u = B^{-1}(u^{(b)}-u^{(c)}) + D_h^T(u^{(c)})R^{-1}(w-h(u^{(c)}))
\end{equation}

to obtain $\Delta u$

3. Update your guess using your favorite optimization algorithm. For example, in steppest descent, choose a learning rate $\eta$ and set 

\begin{equation}
    u_{\text{new}}^{(c)}  = u_{\text{prev}}^{(c)} + \eta\Delta u
\end{equation}

4. Repeat the procedure until $\lvert u_{\text{new}}^{(c)} - u_{\text{prev}}^{(c)} \rvert$ converges to a desired tolerance.


In both the linear and nonlinear case, it should be noted that we have not added time indices to our state vectors. This is an indication that the 3d-var procedure is performed **at every time where you have observation data**.




### 4D-Var

The **3D-Var** algorithm attempts to optimize a cost function to obtain the ideal analysis *for each point where we have observation data*. This can become computationally expensive as we require model evaluations *and* an optimization routine for every observation point. An alternative approach is to simultaneously optimize accross all observations in order to obtain the ideal *initial condition* that acheive the best model fit. This approach is similar to sensitivity analysis which seeks to fit a model's parameters to data. 

To begin, we construct the 4d-var cost function

\begin{align}
    J(u_0) &= \frac{1}{2}\left( u_0 - u_0^{(b)} \right)^TB^{-1}\left( u_0 - u_0^{(b)} \right) + \frac{1}{2}\sum_k\left(w_k - h(u_k) \right)^TR_k^{-1}\left(w_k - h(u_k) \right) \\ 
           &= J_b(u_0) + J_m(u_0)
\end{align}

The first term is usefull if we already have an initial guess $u_0^{(b)}$ for the inital condition in mind. If we do not have one, we may ommit this term. 


As before, we now want to optimize this cost function. To do so, we first observe that 

\begin{equation}
    u_k = \mathcal{M}^{(k)}(u_0; \theta)
\end{equation}

It is easy to obtain the gradient of $J_0$ so we shall focus on the second term. We find that 

\begin{align}
    \nabla_{u_0}J_m &= \nabla_{u_0}\Big\{ \sum_k \frac{1}{2}  \left(w_k - h(u_k) \right)^TR_k^{-1}\left(w_k - h(u_k) \right) \Big\}\\
                    &= - \sum_k \left[\dfrac{\partial }{\partial u_0}h\left(\mathcal{M}^{(k-1)}(u_0)\right) \right]^T R_k^{-1}\left(w_k - h(u_k) \right)\\
                    &= - \sum_k \left[D_h(u_k)D_M(u_{k-1})D_M(u_{k-2})\cdots D_M(u_0) \right]^T R_k^{-1}\left(w_k - h(u_k) \right)\\
                    &= - \sum_k \left[D_M^T(u_0)D_M^T(u_1)\cdots D_M^T(u_{k-1})D_h^T(u_k) \right] R_k^{-1}\left(w_k - h(u_k) \right)\\
\end{align}


Given that we can now obtain the gradient of the cost function, the procedure is nearly identical to 3d-var: 

1. Integrate your model forward to obtain $\{u_k\}$
2. Evaluate each of the $D_M^T(u_{k-1:0})$ and $D_h(u_k)$. 
3. Using these values, compute $\nabla J_m(u)$
4. Set $u_0^{(new)} = u_0^{(prev)} - \eta \nabla J(u_0^{(prev)})$
5. Stop when $\lvert u_0^{(new)} - u_0^{(prev)} \rvert$ converges to your desired tolerance. 

You can of course substitute another optimzation scheme after step 3. 





### Sensitivity Analysis for Differential Equations

Provided some model for a physical system in the form of a set of differential equations, a natural question is: How can we select the parameters for our model in order to get the best possible fit to some experimental data. Similarly, one may wonder what would happen to the prediction of your model if you were to slightly change the values of some parameters. In other words, how *sensitive* is the output of our model to your choice of parameter values? 

In the most general sense, we may frame the problem as follows. Suppose we have a model of the form 

\begin{equation}
    \dfrac{du}{dt} = f(u,t,\theta)
\end{equation}

Given this model, our goal is to optimize a cost function 

\begin{equation}
    J(u; \theta) := \int_0^T g(u;\theta)dt
\end{equation}

where $g(u;\theta)$ is usually taken to be some *quadratic form*. 

As an example, we might consider $g(u\; \theta) = (u(t)-w(t))^T(u(t)-w(t))$ where $w(t)$ denotes the vector of observations at time $t$. 


Our goal then is to find out how $J$ depends on the parameters $\theta$, in other words, to find $\partial J / \partial \theta$. To do this, we will use the method of Lagrange multipliers to generate a so called *adjoint equation* that enables us to find this derivative in a way that minimizes computational cost. As always, this method begins by adding a term that evaluates to 0 into our cost function: 

\begin{equation}
    \mathcal{L} := \int_0^T \left[ g(u;\theta) + \lambda^T(t)\left(f-\dfrac{du}{dt}\right) \right] dt
\end{equation}

From this, we find 

\begin{align}
    \dfrac{\partial \mathcal{L}}{\partial \theta} &:= \int_0^T\left[ \frac{\partial g}{\partial \theta} + \frac{\partial g}{u}\frac{\partial u}{\partial \theta} + \lambda^T(t)\left( \frac{\partial f}{\partial \theta} + \frac{\partial f}{\partial u}\frac{\partial u}{\partial \theta} - \frac{d}{dt}\frac{\partial u}{\partial \theta} \right)\right]dt \\ 
    &= \int_0^T \left[ \frac{\partial g}{\partial \theta} + \lambda^T(t)\frac{\partial f}{\partial \theta} + \left( \frac{\partial g}{\partial u} + \lambda^T(t)\frac{\partial f}{\partial u} - \lambda^T(t)\frac{d}{dt} \right)\frac{\partial u}{\partial \theta} \right]dt
\end{align}

This reorganization is nice because the term $\partial u/\partial \theta$ is the one thats *hard* to compute. Therefore, if we can make the terms in the paretheses evaluate to 0, we will be able to remove this pesky term. Let's use integration by parts to further rearrange by moving the $d/dt$.

\begin{align}
    \int_0^T-\lambda^T(t)\frac{d}{dt}\frac{\partial u}{\partial \theta} dt &= \left[-\lambda^T(t)\frac{\partial u}{\partial \theta} \right]_0^T + \int_0^T \frac{d\lambda^T(t)}{dt}\frac{\partial u}{\partial \theta}dt \\ 
    &= \lambda^T(0)\frac{\partial u_0}{\partial \theta} - \lambda^T(T)\frac{\partial u(T)}{\partial \theta} + \int_0^T \left[ \frac{d\lambda}{dt} \right]^T\frac{\partial u}{\partial \theta}dt
\end{align}

so that plugging this back into our expression for $\partial \mathcal{L}//\partial \theta$, we obtain

\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \theta} = \int_0^T \left[ \frac{\partial g}{\partial \theta} + \lambda^T\frac{\partial f}{\partial \theta} + \left( \frac{\partial g}{\partial u} + \lambda^T\frac{\partial f}{\partial u} + \left[\frac{d\lambda}{dt}\right]^T \right)\frac{\partial u}{\partial \theta}\right]dt + \lambda^T(0)\frac{\partial u_0}{\partial \theta} - \lambda^T(T)\frac{\partial u(T)}{\partial \theta}
\end{equation}

Thus, forcing the nasty terms to dissappear is equivalent find the $\lambda(t)$ subject to the differential equations 

\begin{align}
   \frac{\partial g}{\partial u} + \lambda^T(t)\frac{\partial f}{\partial u} + \frac{d\lambda^T(t)}{dt} &= 0 \\ 
   \lambda^T(T) &= 0
\end{align}

or by taking the transpose: 

\begin{align}
    \frac{d}{dt}\lambda &= - \left[ \frac{\partial g}{\partial u} \right]^T - \left[ \frac{\partial f}{\partial u} \right]^T\lambda  \\ 
    \lambda(T) &= 0
\end{align}


#### Summary
To find the sensitivities $\partial J/\partial \theta$, we perform the following: 

1. Integrate the model $du/dt = f(u,t,\theta)$ forward to obtain $u(t)$.
2. Integrate the adjoint model $d\lambda/dt = -(\partial f/ \partial u)^T\lambda - (\partial g / partial u)^T$ backwards in time from $T$ to $0$ to obtain $\lambda(t)$.
3. Evaluate $\partial J / \partial \theta = \int_0^T\left( \partial g/ \partial \theta + \lambda^T \partial f/\partial \theta\right)dt + \lambda^T(0)\partial u_0/\partial \theta$
